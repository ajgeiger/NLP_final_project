{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c55289a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk, copy\n",
    "import pandas as pd, numpy as np\n",
    "\n",
    "# setup matlib plots\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "\n",
    "# import the MNB module\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# define all stop words\n",
    "stopwordsToRemove = stopwords.words(\"english\")\n",
    "mpl.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d03a76a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# enter the root path here\n",
    "rootPath = \"\"\n",
    "\n",
    "# load in the dataframes\n",
    "train_df = pd.read_table(rootPath+\"corpus/train.tsv\",sep=\"\\t\")\n",
    "test_df = pd.read_table(rootPath+\"corpus/test.tsv\",sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aeb7ae9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Sentiment Score')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "(train_df.groupby(\"Sentiment\").agg(dict(Phrase=\"count\"))/train_df.shape[0]).\\\n",
    "    plot(kind='bar',figsize=(12,5));\n",
    "plt.title(\"distribution of movie sentiment\",fontsize=18);\n",
    "plt.ylabel(\"Percent\")\n",
    "plt.xlabel(\"Sentiment Score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2993e05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a stemmer and lemmatizer (both have been tried)\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "# Remove all tokens of a particular POS tag\n",
    "def removePosToken(phrase, remove_pos):\n",
    "\n",
    "    # convert the string into tokens\n",
    "    tokens = nltk.word_tokenize(phrase)\n",
    "    \n",
    "    # tag each token in the list of strings (Token)\n",
    "    posTokens = nltk.pos_tag(tokens)\n",
    "\n",
    "    # setup the list for the tokens have tokens of a particular\n",
    "    # POS is removed from the original list\n",
    "    modifiedtokens = []\n",
    "    \n",
    "    # itterate through all the pos tokens\n",
    "    for posToken in posTokens:\n",
    "        \n",
    "        # check if the POS token is not the one we are trying to remove\n",
    "        if remove_pos not in posToken[1]: \n",
    "            \n",
    "            # add the good token to the list\n",
    "            modifiedtokens.append(posToken[0])\n",
    "            \n",
    "    # join all the strings in the list back to one  big string\n",
    "    return \" \".join(modifiedtokens)\n",
    "\n",
    "# remove all the stop words\n",
    "def removeStopWords(phrase):\n",
    "    \n",
    "    # removes all the strings in the list phrase that is a stop word\n",
    "    return \" \".join([ w for w in phrase if w not in stopwordsToRemove])\n",
    "\n",
    "# apply the stemmer to the data to reduce the feature size\n",
    "def applyStemmer(x):\n",
    "    \n",
    "    # tokenize the words\n",
    "    tokens = nltk.word_tokenize(x)\n",
    "    \n",
    "    # apply the stemmer\n",
    "    tokens = [stemmer.stem(y) for y in tokens]\n",
    "    \n",
    "    # create a string from the tokens\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "\n",
    "# loop through both the \n",
    "for df in [train_df,test_df]:\n",
    "    \n",
    "    # remove all words with the POS tag RB\n",
    "    df[\"feature_0\"] = df.Phrase.apply(lambda x: removePosToken(x,\"DT\"))\n",
    "    \n",
    "    # here we will lowercase each word and than after, we will remove stop words\n",
    "    df['feature_0'] = df['feature_0'].str.lower().apply(lambda x: removeStopWords(x.split(\" \")))\n",
    "\n",
    "    # apply stemmer or the lemmatizer in this case we ended up going with the lemmatizer\n",
    "    df['feature_0'] = df['feature_0'].apply(lambda x: applyStemmer(x))\n",
    "    \n",
    "    # here we will lowercase each word and than after, we will remove stop words\n",
    "    df['features_1'] = df['Phrase'].str.lower().apply(lambda x: removeStopWords(x.split(\" \")))\n",
    "\n",
    "    # apply stemmer or the lemmatizer in this case we ended up going with the lemmatizer\n",
    "    df['features_1'] = df['features_1'].apply(lambda x: applyStemmer(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f94dcf47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# setup a dataframe to track the fold information\n",
    "foldTracker = pd.DataFrame()\n",
    "validationUpload = dict()\n",
    "\n",
    "kf = KFold(n_splits=10, random_state=1234, shuffle=True)\n",
    "fold_n = 0\n",
    "\n",
    "# create the feature to process\n",
    "for tr_idx, te_idx in kf.split(train_df[\"Phrase\"],train_df[\"Sentiment\"]):\n",
    "    fold_n += 1\n",
    "    for feature_cat in [\"feature_0\",\"features_1\"]:\n",
    "        \n",
    "        # create the train and test variables here for the model\n",
    "        tr_X, tr_y = train_df.loc[tr_idx,feature_cat], train_df.loc[tr_idx,\"Sentiment\"]\n",
    "        te_X, te_y = train_df.loc[te_idx,feature_cat], train_df.loc[te_idx,\"Sentiment\"]\n",
    "        \n",
    "        # setup the unigram count vectorizer\n",
    "        unigram_count_vectorizer =   CountVectorizer(decode_error = 'strict',\n",
    "                                                     ngram_range=(1,1),\n",
    "                                                     lowercase=True,\n",
    "                                                     stop_words='english')\n",
    "\n",
    "        # setup the unigram count vectorizer\n",
    "        bigram_count_vectorizer = CountVectorizer(decode_error = 'strict',\n",
    "                                                  ngram_range=(2,2),\n",
    "                                                  lowercase=True,\n",
    "                                                  stop_words='english')\n",
    "        \n",
    "        # setup the unigram count vectorizer\n",
    "        unigram_bigram_count_vectorizer = CountVectorizer(decode_error = 'strict',\n",
    "                                                          ngram_range=(1,2),\n",
    "                                                          lowercase=True,\n",
    "                                                          stop_words='english')\n",
    "        \n",
    "        for vecType, vectorizer in [(\"unigram\",unigram_count_vectorizer), \n",
    "                                    (\"bigram\",bigram_count_vectorizer),\n",
    "                                   (\"unigram and bigram\", unigram_bigram_count_vectorizer)]:\n",
    "            \n",
    "            # fit the unigram features\n",
    "            vectorizer.fit(tr_X);\n",
    "\n",
    "            # transform the test and train data into the features\n",
    "            uni_features = vectorizer.transform(tr_X);\n",
    "            uni_test_features = vectorizer.transform(te_X);\n",
    "            uni_validation_features = vectorizer.transform(test_df[feature_cat]);\n",
    "\n",
    "            # instatiate the multinomial Naive bayes model\n",
    "            mdl = MultinomialNB(alpha=2)\n",
    "\n",
    "            # fit the multinomial Naive bayes model\n",
    "            mdl.fit(uni_features, tr_y)\n",
    "\n",
    "            # print the accuracy\n",
    "            y_tr_preds = mdl.predict(uni_test_features)\n",
    "            \n",
    "            # setup a dictionary to store all the KPIs In\n",
    "            kpis = dict()\n",
    "            \n",
    "            # Capture all the KPIs related to the evaluation of the model\n",
    "            kpis[\"accuracy\"] = [accuracy_score(te_y, y_tr_preds)]\n",
    "            kpis[\"precision\"] = [precision_score(te_y, y_tr_preds,average=\"weighted\")]           \n",
    "            kpis[\"recall\"] = [recall_score(te_y, y_tr_preds,average=\"weighted\")]                 \n",
    "            kpis[\"f1_score\"] = [f1_score(te_y, y_tr_preds,average=\"weighted\")]                       \n",
    "            kpis[\"feature\"] = [feature_cat]\n",
    "            kpis[\"ngram\"] = [vecType]\n",
    "            kpis[\"data\"]  = [\"Validation\"]\n",
    "            kpis[\"foldn\"]  = [fold_n]\n",
    "            \n",
    "            # create the string which save the probabilities for the sentiment\n",
    "            validationString = feature_cat + \" \" + vecType\n",
    "            \n",
    "            # keep updating the sentiment probabilities\n",
    "            if validationString not in validationUpload:\n",
    "                # setup the initial predictions\n",
    "                validationUpload[validationString] = mdl.predict_proba(uni_validation_features)\n",
    "            else:\n",
    "                # add new probabilities to the previous probabilties\n",
    "                validationUpload[validationString] += mdl.predict_proba(uni_validation_features)\n",
    "            \n",
    "            # upload the data into the table foldTracker\n",
    "            foldTracker = pd.concat([foldTracker, pd.DataFrame(kpis)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2629cf9",
   "metadata": {},
   "source": [
    "## Evaluate each of the feature ngram combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "64d1d385",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>ngram</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>foldn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>feature_0</td>\n",
       "      <td>unigram and bigram</td>\n",
       "      <td>0.629847</td>\n",
       "      <td>0.617574</td>\n",
       "      <td>0.629847</td>\n",
       "      <td>0.619235</td>\n",
       "      <td>5.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>features_1</td>\n",
       "      <td>unigram and bigram</td>\n",
       "      <td>0.629726</td>\n",
       "      <td>0.617484</td>\n",
       "      <td>0.629726</td>\n",
       "      <td>0.619159</td>\n",
       "      <td>5.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>feature_0</td>\n",
       "      <td>unigram</td>\n",
       "      <td>0.609496</td>\n",
       "      <td>0.589095</td>\n",
       "      <td>0.609496</td>\n",
       "      <td>0.585617</td>\n",
       "      <td>5.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>features_1</td>\n",
       "      <td>unigram</td>\n",
       "      <td>0.609471</td>\n",
       "      <td>0.589060</td>\n",
       "      <td>0.609471</td>\n",
       "      <td>0.585615</td>\n",
       "      <td>5.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>features_1</td>\n",
       "      <td>bigram</td>\n",
       "      <td>0.608849</td>\n",
       "      <td>0.590168</td>\n",
       "      <td>0.608849</td>\n",
       "      <td>0.581690</td>\n",
       "      <td>5.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>feature_0</td>\n",
       "      <td>bigram</td>\n",
       "      <td>0.608836</td>\n",
       "      <td>0.590162</td>\n",
       "      <td>0.608836</td>\n",
       "      <td>0.581549</td>\n",
       "      <td>5.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      feature               ngram  accuracy  precision    recall  f1_score  \\\n",
       "0   feature_0  unigram and bigram  0.629847   0.617574  0.629847  0.619235   \n",
       "1  features_1  unigram and bigram  0.629726   0.617484  0.629726  0.619159   \n",
       "2   feature_0             unigram  0.609496   0.589095  0.609496  0.585617   \n",
       "3  features_1             unigram  0.609471   0.589060  0.609471  0.585615   \n",
       "4  features_1              bigram  0.608849   0.590168  0.608849  0.581690   \n",
       "5   feature_0              bigram  0.608836   0.590162  0.608836  0.581549   \n",
       "\n",
       "   foldn  \n",
       "0    5.5  \n",
       "1    5.5  \n",
       "2    5.5  \n",
       "3    5.5  \n",
       "4    5.5  \n",
       "5    5.5  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analysisResults = foldTracker.groupby([\"feature\",\"ngram\"]).mean()\\\n",
    "            .reset_index()\\\n",
    "                .sort_values(\"f1_score\",ascending=False)\\\n",
    "                    .reset_index(drop=True)\n",
    "\n",
    "analysisResults"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b746c8c",
   "metadata": {},
   "source": [
    "## Create predictions for the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "96abc0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the best set\n",
    "bestSet = validationUpload[analysisResults.loc[0,\"feature\"] + \" \" + analysisResults.loc[0,\"ngram\"]]\n",
    "\n",
    "# create the dataframe\n",
    "bestSet_df = pd.DataFrame(bestSet)\n",
    "\n",
    "# figure out which is the most likely answer for the valudation value\n",
    "def computeAnswer(x):\n",
    "    \n",
    "    # get the soft max distribution\n",
    "    vals = x.values\n",
    "    \n",
    "    # get the cols assocated to each of the soft max values\n",
    "    keys = np.array(list(x.index))\n",
    "    \n",
    "    return np.max(keys[vals == np.max(vals)])\n",
    "\n",
    "# figure out what the most likely sentiment is\n",
    "bestSet_df[\"prediction\"] = bestSet_df.apply(lambda x: computeAnswer(x),axis=1)\n",
    "\n",
    "# save the results to a CSV file\n",
    "bestSet_df.to_csv(\"predictions.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "27348c48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.006441</td>\n",
       "      <td>0.621994</td>\n",
       "      <td>4.148558</td>\n",
       "      <td>5.193696</td>\n",
       "      <td>0.029312</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.006441</td>\n",
       "      <td>0.621994</td>\n",
       "      <td>4.148558</td>\n",
       "      <td>5.193696</td>\n",
       "      <td>0.029312</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.453159</td>\n",
       "      <td>1.747597</td>\n",
       "      <td>5.099449</td>\n",
       "      <td>2.109894</td>\n",
       "      <td>0.589901</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.006441</td>\n",
       "      <td>0.621994</td>\n",
       "      <td>4.148558</td>\n",
       "      <td>5.193696</td>\n",
       "      <td>0.029312</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.021089</td>\n",
       "      <td>0.729925</td>\n",
       "      <td>4.218588</td>\n",
       "      <td>4.943903</td>\n",
       "      <td>0.086494</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4  prediction\n",
       "0  0.006441  0.621994  4.148558  5.193696  0.029312           3\n",
       "1  0.006441  0.621994  4.148558  5.193696  0.029312           3\n",
       "2  0.453159  1.747597  5.099449  2.109894  0.589901           2\n",
       "3  0.006441  0.621994  4.148558  5.193696  0.029312           3\n",
       "4  0.021089  0.729925  4.218588  4.943903  0.086494           3"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bestSet_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986552b4",
   "metadata": {},
   "source": [
    "## Display Fold KPIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "95c4ed16",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>feature</th>\n",
       "      <th>ngram</th>\n",
       "      <th>data</th>\n",
       "      <th>foldn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.632834</td>\n",
       "      <td>0.620724</td>\n",
       "      <td>0.632834</td>\n",
       "      <td>0.622818</td>\n",
       "      <td>feature_0</td>\n",
       "      <td>unigram and bigram</td>\n",
       "      <td>Validation</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.632641</td>\n",
       "      <td>0.620526</td>\n",
       "      <td>0.632641</td>\n",
       "      <td>0.622610</td>\n",
       "      <td>features_1</td>\n",
       "      <td>unigram and bigram</td>\n",
       "      <td>Validation</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.633859</td>\n",
       "      <td>0.620258</td>\n",
       "      <td>0.633859</td>\n",
       "      <td>0.622447</td>\n",
       "      <td>features_1</td>\n",
       "      <td>unigram and bigram</td>\n",
       "      <td>Validation</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.633731</td>\n",
       "      <td>0.620166</td>\n",
       "      <td>0.633731</td>\n",
       "      <td>0.622332</td>\n",
       "      <td>feature_0</td>\n",
       "      <td>unigram and bigram</td>\n",
       "      <td>Validation</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.631616</td>\n",
       "      <td>0.619815</td>\n",
       "      <td>0.631616</td>\n",
       "      <td>0.622070</td>\n",
       "      <td>feature_0</td>\n",
       "      <td>unigram and bigram</td>\n",
       "      <td>Validation</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.631424</td>\n",
       "      <td>0.619751</td>\n",
       "      <td>0.631424</td>\n",
       "      <td>0.622011</td>\n",
       "      <td>features_1</td>\n",
       "      <td>unigram and bigram</td>\n",
       "      <td>Validation</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.633410</td>\n",
       "      <td>0.620361</td>\n",
       "      <td>0.633410</td>\n",
       "      <td>0.621856</td>\n",
       "      <td>feature_0</td>\n",
       "      <td>unigram and bigram</td>\n",
       "      <td>Validation</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.632962</td>\n",
       "      <td>0.619883</td>\n",
       "      <td>0.632962</td>\n",
       "      <td>0.621426</td>\n",
       "      <td>features_1</td>\n",
       "      <td>unigram and bigram</td>\n",
       "      <td>Validation</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.630783</td>\n",
       "      <td>0.619083</td>\n",
       "      <td>0.630783</td>\n",
       "      <td>0.620624</td>\n",
       "      <td>features_1</td>\n",
       "      <td>unigram and bigram</td>\n",
       "      <td>Validation</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.630591</td>\n",
       "      <td>0.618890</td>\n",
       "      <td>0.630591</td>\n",
       "      <td>0.620398</td>\n",
       "      <td>feature_0</td>\n",
       "      <td>unigram and bigram</td>\n",
       "      <td>Validation</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.628925</td>\n",
       "      <td>0.616723</td>\n",
       "      <td>0.628925</td>\n",
       "      <td>0.618083</td>\n",
       "      <td>feature_0</td>\n",
       "      <td>unigram and bigram</td>\n",
       "      <td>Validation</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.628733</td>\n",
       "      <td>0.616506</td>\n",
       "      <td>0.628733</td>\n",
       "      <td>0.617952</td>\n",
       "      <td>features_1</td>\n",
       "      <td>unigram and bigram</td>\n",
       "      <td>Validation</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.628284</td>\n",
       "      <td>0.615270</td>\n",
       "      <td>0.628284</td>\n",
       "      <td>0.617246</td>\n",
       "      <td>feature_0</td>\n",
       "      <td>unigram and bigram</td>\n",
       "      <td>Validation</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.628028</td>\n",
       "      <td>0.615333</td>\n",
       "      <td>0.628028</td>\n",
       "      <td>0.617209</td>\n",
       "      <td>feature_0</td>\n",
       "      <td>unigram and bigram</td>\n",
       "      <td>Validation</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.627964</td>\n",
       "      <td>0.614969</td>\n",
       "      <td>0.627964</td>\n",
       "      <td>0.616987</td>\n",
       "      <td>features_1</td>\n",
       "      <td>unigram and bigram</td>\n",
       "      <td>Validation</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.627643</td>\n",
       "      <td>0.615071</td>\n",
       "      <td>0.627643</td>\n",
       "      <td>0.616930</td>\n",
       "      <td>features_1</td>\n",
       "      <td>unigram and bigram</td>\n",
       "      <td>Validation</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.625272</td>\n",
       "      <td>0.614547</td>\n",
       "      <td>0.625272</td>\n",
       "      <td>0.615363</td>\n",
       "      <td>features_1</td>\n",
       "      <td>unigram and bigram</td>\n",
       "      <td>Validation</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.625977</td>\n",
       "      <td>0.614248</td>\n",
       "      <td>0.625977</td>\n",
       "      <td>0.615241</td>\n",
       "      <td>features_1</td>\n",
       "      <td>unigram and bigram</td>\n",
       "      <td>Validation</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.625144</td>\n",
       "      <td>0.614313</td>\n",
       "      <td>0.625144</td>\n",
       "      <td>0.615194</td>\n",
       "      <td>feature_0</td>\n",
       "      <td>unigram and bigram</td>\n",
       "      <td>Validation</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.625913</td>\n",
       "      <td>0.614146</td>\n",
       "      <td>0.625913</td>\n",
       "      <td>0.615143</td>\n",
       "      <td>feature_0</td>\n",
       "      <td>unigram and bigram</td>\n",
       "      <td>Validation</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.617967</td>\n",
       "      <td>0.599779</td>\n",
       "      <td>0.617967</td>\n",
       "      <td>0.596482</td>\n",
       "      <td>feature_0</td>\n",
       "      <td>unigram</td>\n",
       "      <td>Validation</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.617583</td>\n",
       "      <td>0.599262</td>\n",
       "      <td>0.617583</td>\n",
       "      <td>0.596156</td>\n",
       "      <td>features_1</td>\n",
       "      <td>unigram</td>\n",
       "      <td>Validation</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.614507</td>\n",
       "      <td>0.595857</td>\n",
       "      <td>0.614507</td>\n",
       "      <td>0.592017</td>\n",
       "      <td>features_1</td>\n",
       "      <td>unigram</td>\n",
       "      <td>Validation</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.614315</td>\n",
       "      <td>0.595701</td>\n",
       "      <td>0.614315</td>\n",
       "      <td>0.591840</td>\n",
       "      <td>feature_0</td>\n",
       "      <td>unigram</td>\n",
       "      <td>Validation</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.611880</td>\n",
       "      <td>0.590977</td>\n",
       "      <td>0.611880</td>\n",
       "      <td>0.587418</td>\n",
       "      <td>feature_0</td>\n",
       "      <td>unigram</td>\n",
       "      <td>Validation</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.611624</td>\n",
       "      <td>0.590586</td>\n",
       "      <td>0.611624</td>\n",
       "      <td>0.587069</td>\n",
       "      <td>features_1</td>\n",
       "      <td>unigram</td>\n",
       "      <td>Validation</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.611367</td>\n",
       "      <td>0.589115</td>\n",
       "      <td>0.611367</td>\n",
       "      <td>0.586815</td>\n",
       "      <td>feature_0</td>\n",
       "      <td>unigram</td>\n",
       "      <td>Validation</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.611303</td>\n",
       "      <td>0.589065</td>\n",
       "      <td>0.611303</td>\n",
       "      <td>0.586796</td>\n",
       "      <td>features_1</td>\n",
       "      <td>unigram</td>\n",
       "      <td>Validation</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.614635</td>\n",
       "      <td>0.594438</td>\n",
       "      <td>0.614635</td>\n",
       "      <td>0.585436</td>\n",
       "      <td>features_1</td>\n",
       "      <td>bigram</td>\n",
       "      <td>Validation</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.611624</td>\n",
       "      <td>0.595095</td>\n",
       "      <td>0.611624</td>\n",
       "      <td>0.585250</td>\n",
       "      <td>features_1</td>\n",
       "      <td>bigram</td>\n",
       "      <td>Validation</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.614571</td>\n",
       "      <td>0.594345</td>\n",
       "      <td>0.614571</td>\n",
       "      <td>0.585179</td>\n",
       "      <td>feature_0</td>\n",
       "      <td>bigram</td>\n",
       "      <td>Validation</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.611688</td>\n",
       "      <td>0.595229</td>\n",
       "      <td>0.611688</td>\n",
       "      <td>0.585166</td>\n",
       "      <td>feature_0</td>\n",
       "      <td>bigram</td>\n",
       "      <td>Validation</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.609573</td>\n",
       "      <td>0.587351</td>\n",
       "      <td>0.609573</td>\n",
       "      <td>0.584580</td>\n",
       "      <td>features_1</td>\n",
       "      <td>unigram</td>\n",
       "      <td>Validation</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.607587</td>\n",
       "      <td>0.587978</td>\n",
       "      <td>0.607587</td>\n",
       "      <td>0.584511</td>\n",
       "      <td>features_1</td>\n",
       "      <td>unigram</td>\n",
       "      <td>Validation</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.608868</td>\n",
       "      <td>0.587029</td>\n",
       "      <td>0.608868</td>\n",
       "      <td>0.584433</td>\n",
       "      <td>feature_0</td>\n",
       "      <td>unigram</td>\n",
       "      <td>Validation</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.608676</td>\n",
       "      <td>0.586817</td>\n",
       "      <td>0.608676</td>\n",
       "      <td>0.584196</td>\n",
       "      <td>features_1</td>\n",
       "      <td>unigram</td>\n",
       "      <td>Validation</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.607395</td>\n",
       "      <td>0.587706</td>\n",
       "      <td>0.607395</td>\n",
       "      <td>0.584171</td>\n",
       "      <td>feature_0</td>\n",
       "      <td>unigram</td>\n",
       "      <td>Validation</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.611047</td>\n",
       "      <td>0.590203</td>\n",
       "      <td>0.611047</td>\n",
       "      <td>0.584088</td>\n",
       "      <td>features_1</td>\n",
       "      <td>bigram</td>\n",
       "      <td>Validation</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.609125</td>\n",
       "      <td>0.586823</td>\n",
       "      <td>0.609125</td>\n",
       "      <td>0.584015</td>\n",
       "      <td>feature_0</td>\n",
       "      <td>unigram</td>\n",
       "      <td>Validation</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.610855</td>\n",
       "      <td>0.589997</td>\n",
       "      <td>0.610855</td>\n",
       "      <td>0.583810</td>\n",
       "      <td>feature_0</td>\n",
       "      <td>bigram</td>\n",
       "      <td>Validation</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.611303</td>\n",
       "      <td>0.593919</td>\n",
       "      <td>0.611303</td>\n",
       "      <td>0.583220</td>\n",
       "      <td>feature_0</td>\n",
       "      <td>bigram</td>\n",
       "      <td>Validation</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.611175</td>\n",
       "      <td>0.593712</td>\n",
       "      <td>0.611175</td>\n",
       "      <td>0.583209</td>\n",
       "      <td>features_1</td>\n",
       "      <td>bigram</td>\n",
       "      <td>Validation</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.610534</td>\n",
       "      <td>0.590505</td>\n",
       "      <td>0.610534</td>\n",
       "      <td>0.583080</td>\n",
       "      <td>feature_0</td>\n",
       "      <td>bigram</td>\n",
       "      <td>Validation</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.610342</td>\n",
       "      <td>0.590241</td>\n",
       "      <td>0.610342</td>\n",
       "      <td>0.582944</td>\n",
       "      <td>features_1</td>\n",
       "      <td>bigram</td>\n",
       "      <td>Validation</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.605344</td>\n",
       "      <td>0.585573</td>\n",
       "      <td>0.605344</td>\n",
       "      <td>0.582342</td>\n",
       "      <td>features_1</td>\n",
       "      <td>unigram</td>\n",
       "      <td>Validation</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.605280</td>\n",
       "      <td>0.585450</td>\n",
       "      <td>0.605280</td>\n",
       "      <td>0.582212</td>\n",
       "      <td>feature_0</td>\n",
       "      <td>unigram</td>\n",
       "      <td>Validation</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.607459</td>\n",
       "      <td>0.590021</td>\n",
       "      <td>0.607459</td>\n",
       "      <td>0.580616</td>\n",
       "      <td>features_1</td>\n",
       "      <td>bigram</td>\n",
       "      <td>Validation</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.606433</td>\n",
       "      <td>0.588485</td>\n",
       "      <td>0.606433</td>\n",
       "      <td>0.580443</td>\n",
       "      <td>features_1</td>\n",
       "      <td>bigram</td>\n",
       "      <td>Validation</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.606498</td>\n",
       "      <td>0.588452</td>\n",
       "      <td>0.606498</td>\n",
       "      <td>0.580343</td>\n",
       "      <td>features_1</td>\n",
       "      <td>bigram</td>\n",
       "      <td>Validation</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.607395</td>\n",
       "      <td>0.589903</td>\n",
       "      <td>0.607395</td>\n",
       "      <td>0.580301</td>\n",
       "      <td>feature_0</td>\n",
       "      <td>bigram</td>\n",
       "      <td>Validation</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>0.606369</td>\n",
       "      <td>0.588358</td>\n",
       "      <td>0.606369</td>\n",
       "      <td>0.580267</td>\n",
       "      <td>feature_0</td>\n",
       "      <td>bigram</td>\n",
       "      <td>Validation</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>0.605600</td>\n",
       "      <td>0.585348</td>\n",
       "      <td>0.605600</td>\n",
       "      <td>0.579959</td>\n",
       "      <td>features_1</td>\n",
       "      <td>unigram</td>\n",
       "      <td>Validation</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>0.606177</td>\n",
       "      <td>0.588142</td>\n",
       "      <td>0.606177</td>\n",
       "      <td>0.579912</td>\n",
       "      <td>feature_0</td>\n",
       "      <td>bigram</td>\n",
       "      <td>Validation</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>0.605408</td>\n",
       "      <td>0.585193</td>\n",
       "      <td>0.605408</td>\n",
       "      <td>0.579795</td>\n",
       "      <td>feature_0</td>\n",
       "      <td>unigram</td>\n",
       "      <td>Validation</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>0.603358</td>\n",
       "      <td>0.583176</td>\n",
       "      <td>0.603358</td>\n",
       "      <td>0.578983</td>\n",
       "      <td>feature_0</td>\n",
       "      <td>unigram</td>\n",
       "      <td>Validation</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>0.602909</td>\n",
       "      <td>0.582760</td>\n",
       "      <td>0.602909</td>\n",
       "      <td>0.578521</td>\n",
       "      <td>features_1</td>\n",
       "      <td>unigram</td>\n",
       "      <td>Validation</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>0.606241</td>\n",
       "      <td>0.586156</td>\n",
       "      <td>0.606241</td>\n",
       "      <td>0.578449</td>\n",
       "      <td>feature_0</td>\n",
       "      <td>bigram</td>\n",
       "      <td>Validation</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>0.606049</td>\n",
       "      <td>0.585898</td>\n",
       "      <td>0.606049</td>\n",
       "      <td>0.578334</td>\n",
       "      <td>features_1</td>\n",
       "      <td>bigram</td>\n",
       "      <td>Validation</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>0.603230</td>\n",
       "      <td>0.585131</td>\n",
       "      <td>0.603230</td>\n",
       "      <td>0.576236</td>\n",
       "      <td>features_1</td>\n",
       "      <td>bigram</td>\n",
       "      <td>Validation</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>0.603230</td>\n",
       "      <td>0.585069</td>\n",
       "      <td>0.603230</td>\n",
       "      <td>0.576105</td>\n",
       "      <td>feature_0</td>\n",
       "      <td>bigram</td>\n",
       "      <td>Validation</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    accuracy  precision    recall  f1_score     feature               ngram  \\\n",
       "0   0.632834   0.620724  0.632834  0.622818   feature_0  unigram and bigram   \n",
       "1   0.632641   0.620526  0.632641  0.622610  features_1  unigram and bigram   \n",
       "2   0.633859   0.620258  0.633859  0.622447  features_1  unigram and bigram   \n",
       "3   0.633731   0.620166  0.633731  0.622332   feature_0  unigram and bigram   \n",
       "4   0.631616   0.619815  0.631616  0.622070   feature_0  unigram and bigram   \n",
       "5   0.631424   0.619751  0.631424  0.622011  features_1  unigram and bigram   \n",
       "6   0.633410   0.620361  0.633410  0.621856   feature_0  unigram and bigram   \n",
       "7   0.632962   0.619883  0.632962  0.621426  features_1  unigram and bigram   \n",
       "8   0.630783   0.619083  0.630783  0.620624  features_1  unigram and bigram   \n",
       "9   0.630591   0.618890  0.630591  0.620398   feature_0  unigram and bigram   \n",
       "10  0.628925   0.616723  0.628925  0.618083   feature_0  unigram and bigram   \n",
       "11  0.628733   0.616506  0.628733  0.617952  features_1  unigram and bigram   \n",
       "12  0.628284   0.615270  0.628284  0.617246   feature_0  unigram and bigram   \n",
       "13  0.628028   0.615333  0.628028  0.617209   feature_0  unigram and bigram   \n",
       "14  0.627964   0.614969  0.627964  0.616987  features_1  unigram and bigram   \n",
       "15  0.627643   0.615071  0.627643  0.616930  features_1  unigram and bigram   \n",
       "16  0.625272   0.614547  0.625272  0.615363  features_1  unigram and bigram   \n",
       "17  0.625977   0.614248  0.625977  0.615241  features_1  unigram and bigram   \n",
       "18  0.625144   0.614313  0.625144  0.615194   feature_0  unigram and bigram   \n",
       "19  0.625913   0.614146  0.625913  0.615143   feature_0  unigram and bigram   \n",
       "20  0.617967   0.599779  0.617967  0.596482   feature_0             unigram   \n",
       "21  0.617583   0.599262  0.617583  0.596156  features_1             unigram   \n",
       "22  0.614507   0.595857  0.614507  0.592017  features_1             unigram   \n",
       "23  0.614315   0.595701  0.614315  0.591840   feature_0             unigram   \n",
       "24  0.611880   0.590977  0.611880  0.587418   feature_0             unigram   \n",
       "25  0.611624   0.590586  0.611624  0.587069  features_1             unigram   \n",
       "26  0.611367   0.589115  0.611367  0.586815   feature_0             unigram   \n",
       "27  0.611303   0.589065  0.611303  0.586796  features_1             unigram   \n",
       "28  0.614635   0.594438  0.614635  0.585436  features_1              bigram   \n",
       "29  0.611624   0.595095  0.611624  0.585250  features_1              bigram   \n",
       "30  0.614571   0.594345  0.614571  0.585179   feature_0              bigram   \n",
       "31  0.611688   0.595229  0.611688  0.585166   feature_0              bigram   \n",
       "32  0.609573   0.587351  0.609573  0.584580  features_1             unigram   \n",
       "33  0.607587   0.587978  0.607587  0.584511  features_1             unigram   \n",
       "34  0.608868   0.587029  0.608868  0.584433   feature_0             unigram   \n",
       "35  0.608676   0.586817  0.608676  0.584196  features_1             unigram   \n",
       "36  0.607395   0.587706  0.607395  0.584171   feature_0             unigram   \n",
       "37  0.611047   0.590203  0.611047  0.584088  features_1              bigram   \n",
       "38  0.609125   0.586823  0.609125  0.584015   feature_0             unigram   \n",
       "39  0.610855   0.589997  0.610855  0.583810   feature_0              bigram   \n",
       "40  0.611303   0.593919  0.611303  0.583220   feature_0              bigram   \n",
       "41  0.611175   0.593712  0.611175  0.583209  features_1              bigram   \n",
       "42  0.610534   0.590505  0.610534  0.583080   feature_0              bigram   \n",
       "43  0.610342   0.590241  0.610342  0.582944  features_1              bigram   \n",
       "44  0.605344   0.585573  0.605344  0.582342  features_1             unigram   \n",
       "45  0.605280   0.585450  0.605280  0.582212   feature_0             unigram   \n",
       "46  0.607459   0.590021  0.607459  0.580616  features_1              bigram   \n",
       "47  0.606433   0.588485  0.606433  0.580443  features_1              bigram   \n",
       "48  0.606498   0.588452  0.606498  0.580343  features_1              bigram   \n",
       "49  0.607395   0.589903  0.607395  0.580301   feature_0              bigram   \n",
       "50  0.606369   0.588358  0.606369  0.580267   feature_0              bigram   \n",
       "51  0.605600   0.585348  0.605600  0.579959  features_1             unigram   \n",
       "52  0.606177   0.588142  0.606177  0.579912   feature_0              bigram   \n",
       "53  0.605408   0.585193  0.605408  0.579795   feature_0             unigram   \n",
       "54  0.603358   0.583176  0.603358  0.578983   feature_0             unigram   \n",
       "55  0.602909   0.582760  0.602909  0.578521  features_1             unigram   \n",
       "56  0.606241   0.586156  0.606241  0.578449   feature_0              bigram   \n",
       "57  0.606049   0.585898  0.606049  0.578334  features_1              bigram   \n",
       "58  0.603230   0.585131  0.603230  0.576236  features_1              bigram   \n",
       "59  0.603230   0.585069  0.603230  0.576105   feature_0              bigram   \n",
       "\n",
       "          data  foldn  \n",
       "0   Validation      9  \n",
       "1   Validation      9  \n",
       "2   Validation      3  \n",
       "3   Validation      3  \n",
       "4   Validation      4  \n",
       "5   Validation      4  \n",
       "6   Validation      2  \n",
       "7   Validation      2  \n",
       "8   Validation      5  \n",
       "9   Validation      5  \n",
       "10  Validation      8  \n",
       "11  Validation      8  \n",
       "12  Validation      7  \n",
       "13  Validation      1  \n",
       "14  Validation      7  \n",
       "15  Validation      1  \n",
       "16  Validation      6  \n",
       "17  Validation     10  \n",
       "18  Validation      6  \n",
       "19  Validation     10  \n",
       "20  Validation      4  \n",
       "21  Validation      4  \n",
       "22  Validation      9  \n",
       "23  Validation      9  \n",
       "24  Validation      7  \n",
       "25  Validation      7  \n",
       "26  Validation      3  \n",
       "27  Validation      3  \n",
       "28  Validation      3  \n",
       "29  Validation      9  \n",
       "30  Validation      3  \n",
       "31  Validation      9  \n",
       "32  Validation      8  \n",
       "33  Validation      5  \n",
       "34  Validation      1  \n",
       "35  Validation      1  \n",
       "36  Validation      5  \n",
       "37  Validation      4  \n",
       "38  Validation      8  \n",
       "39  Validation      4  \n",
       "40  Validation      2  \n",
       "41  Validation      2  \n",
       "42  Validation      8  \n",
       "43  Validation      8  \n",
       "44  Validation      6  \n",
       "45  Validation      6  \n",
       "46  Validation      7  \n",
       "47  Validation     10  \n",
       "48  Validation      6  \n",
       "49  Validation      7  \n",
       "50  Validation     10  \n",
       "51  Validation      2  \n",
       "52  Validation      6  \n",
       "53  Validation      2  \n",
       "54  Validation     10  \n",
       "55  Validation     10  \n",
       "56  Validation      1  \n",
       "57  Validation      1  \n",
       "58  Validation      5  \n",
       "59  Validation      5  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "foldTracker.sort_values(\"f1_score\",ascending=False).reset_index(drop=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
